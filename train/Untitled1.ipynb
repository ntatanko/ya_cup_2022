{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73913a9f-2f86-41ae-be87-18b796243e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import albumentations as A\n",
    "import annoy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_val_split(df, fold, seed=42):\n",
    "    df = df.merge(\n",
    "        df[\"artistid\"].value_counts(),\n",
    "        left_on=\"artistid\",\n",
    "        right_index=True,\n",
    "        suffixes=[None, \"_count\"],\n",
    "    )\n",
    "    gkf = StratifiedGroupKFold(n_splits=8, shuffle=True, random_state=seed)\n",
    "    for n, (train_ids, val_ids) in enumerate(\n",
    "        gkf.split(\n",
    "            X=df[[\"artistid\", \"artistid_count\"]],\n",
    "            y=df[\"artistid_count\"],\n",
    "            groups=df[\"artistid\"],\n",
    "        )\n",
    "    ):\n",
    "        df.loc[val_ids, \"fold\"] = n\n",
    "    train_df = df[df[\"fold\"] != fold].reset_index(drop=True).copy()\n",
    "    val_df = df[df[\"fold\"] == fold].reset_index(drop=True).copy()\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "class FeaturesLoader:\n",
    "    def __init__(\n",
    "        self, features_dir_path, df, device=\"cpu\", augment=False, crop_size=60\n",
    "    ):\n",
    "        self.features_dir_path = features_dir_path\n",
    "        self.df = df\n",
    "        self.df[\"path\"] = self.df[\"archive_features_path\"].apply(\n",
    "            lambda x: os.path.join(features_dir_path, x)\n",
    "        )\n",
    "        self.trackid2path = df.set_index(\"trackid\")[\"path\"].to_dict()\n",
    "        self.crop_size = crop_size\n",
    "        self.device = device\n",
    "        self.augment = augment\n",
    "\n",
    "    def augment_fn(self, img):\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.RandomCrop(always_apply=True, p=1.0, height=512, width=60),\n",
    "                A.Flip(p=0.2),\n",
    "                A.PixelDropout(p=0.1, dropout_prob=0.01),\n",
    "                A.CoarseDropout(\n",
    "                    p=0.1,\n",
    "                    max_holes=11,\n",
    "                    max_height=5,\n",
    "                    max_width=3,\n",
    "                    min_holes=1,\n",
    "                    min_height=2,\n",
    "                    min_width=2,\n",
    "                ),\n",
    "                A.RandomGridShuffle(p=0.3, grid=(1, 6)),\n",
    "            ]\n",
    "        )\n",
    "        return transform(image=img)[\"image\"]\n",
    "\n",
    "    def _load_item(self, track_id):\n",
    "        img = np.load(self.trackid2path[track_id])\n",
    "        if not self.augment:\n",
    "            padding = (img.shape[1] - self.crop_size) // 2\n",
    "            img = img[:, padding : padding + self.crop_size]\n",
    "        else:\n",
    "            img = self.augment_fn(img)\n",
    "        return img\n",
    "\n",
    "    def load_batch(self, tracks_ids):\n",
    "        batch = [self._load_item(track_id) for track_id in tracks_ids]\n",
    "        return torch.tensor(np.array(batch)).to(self.device)\n",
    "\n",
    "\n",
    "class TrainLoader:\n",
    "    def __init__(self, features_loader, batch_size=256, features_size=(512, 60)):\n",
    "        self.features_loader = features_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.features_size = features_size\n",
    "        self.artist_track_ids = self.features_loader.df.groupby(\"artistid\").agg(list)\n",
    "\n",
    "    def _generate_pairs(self, track_ids):\n",
    "        np.random.shuffle(track_ids)\n",
    "        pairs = [track_ids[i - 2 : i] for i in range(2, len(track_ids) + 1, 2)]\n",
    "        return pairs\n",
    "\n",
    "    def _get_pair_ids(self):\n",
    "        pairs = []\n",
    "        artist_track_ids = self.artist_track_ids.copy()\n",
    "        artist_track_pairs = artist_track_ids[\"trackid\"].map(self._generate_pairs)\n",
    "        for pair_ids in artist_track_pairs.explode().dropna():\n",
    "            pairs.append(pair_ids)\n",
    "        np.random.shuffle(pairs)\n",
    "        return pairs\n",
    "\n",
    "    def _get_batch(self, batch_ids):\n",
    "        batch_ids = np.array(batch_ids).reshape(-1)\n",
    "        batch_features = self.features_loader.load_batch(batch_ids)\n",
    "        batch_features = batch_features.reshape(self.batch_size, 2, *self.features_size)\n",
    "        return batch_features\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_ids = []\n",
    "        for pair_ids in self._get_pair_ids():\n",
    "            batch_ids.append(pair_ids)\n",
    "            if len(batch_ids) == self.batch_size:\n",
    "                batch = self._get_batch(batch_ids)\n",
    "                yield batch\n",
    "                batch_ids = []\n",
    "\n",
    "    def _len(self):\n",
    "        return sum(1 for x in self._get_pair_ids()) // self.batch_size\n",
    "\n",
    "\n",
    "class TestLoader:\n",
    "    def __init__(self, features_loader, batch_size=256, features_size=(512, 60)):\n",
    "        self.features_loader = features_loader\n",
    "        self.batch_size = batch_size\n",
    "        self.features_size = features_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_ids = []\n",
    "        for track_id in tqdm(self.features_loader.df[\"trackid\"].values):\n",
    "            batch_ids.append(track_id)\n",
    "            if len(batch_ids) == self.batch_size:\n",
    "                yield batch_ids, self.features_loader.load_batch(batch_ids)\n",
    "                batch_ids = []\n",
    "        if len(batch_ids) > 0:\n",
    "            yield batch_ids, self.features_loader.load_batch(batch_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1daffeb7-72fe-4b3a-a540-7a7a0036b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_xent_loss(out1, out2, temperature, device):\n",
    "    out = torch.cat([out1, out2], dim=0)\n",
    "    n_samples = len(out)\n",
    "\n",
    "    # Full similarity matrix\n",
    "    cov = torch.mm(out, out.t().contiguous())\n",
    "    sim = torch.exp(cov / temperature)\n",
    "\n",
    "    # Negative similarity\n",
    "    mask = ~torch.eye(n_samples, device=device).bool()\n",
    "    neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
    "\n",
    "    # Positive similarity :\n",
    "    pos = torch.exp(torch.sum(out1 * out2, dim=-1) / temperature)\n",
    "    pos = torch.cat([pos, pos], dim=0)\n",
    "\n",
    "    loss = -torch.log(pos / neg).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1687d847-e356-43fd-b5fb-afc970990f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_metric_learning\n",
      "  Downloading pytorch_metric_learning-1.6.3-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.4/111.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from pytorch_metric_learning) (0.13.1+cu113)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_metric_learning) (1.12.1+cu113)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from pytorch_metric_learning) (1.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_metric_learning) (1.23.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_metric_learning) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->pytorch_metric_learning) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->pytorch_metric_learning) (1.4.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->pytorch_metric_learning) (1.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->pytorch_metric_learning) (9.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->pytorch_metric_learning) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->pytorch_metric_learning) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->pytorch_metric_learning) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->pytorch_metric_learning) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->pytorch_metric_learning) (2.1.1)\n",
      "Installing collected packages: pytorch_metric_learning\n",
      "Successfully installed pytorch_metric_learning-1.6.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_metric_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29e7e0fc-9b69-41be-82d4-830666b76529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning.losses import NTXentLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2475fd28-72c1-4488-a5aa-70ad13fc84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = NTXentLoss(temperature=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44825d4a-bffa-4e07-b373-adb768cbd6a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels must be a 1D tensor of shape (batch_size,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_metric_learning/losses/base_metric_loss_function.py:34\u001b[0m, in \u001b[0;36mBaseMetricLossFunction.forward\u001b[0;34m(self, embeddings, labels, indices_tuple, ref_emb, ref_labels)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    embeddings: tensor of size (batch_size, embedding_size)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mReturns: the loss\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_stats()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mc_f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     labels \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mto_device(labels, embeddings)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch_metric_learning/utils/common_functions.py:420\u001b[0m, in \u001b[0;36mcheck_shapes\u001b[0;34m(embeddings, labels)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings must be a 2D tensor of shape (batch_size, embedding_size)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels must be a 1D tensor of shape (batch_size,)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: labels must be a 1D tensor of shape (batch_size,)"
     ]
    }
   ],
   "source": [
    "l(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d79803c-8ef1-4ace-bfdb-8072c9b635c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6960, dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt_xent_loss(a, a, 0.1, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0e1170-c779-42f2-9d16-1c11e22167b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(np.random.random([4,20]))\n",
    "b = torch.tensor(np.random.random([4,20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24387fff-41f7-42c7-96f2-632db17403a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.cat([a, a], dim=0)\n",
    "n_samples = len(out)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41a58fab-4f70-4e01-a599-fe5922dac8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = torch.mm(out, out.t().contiguous())\n",
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ad098b3-cce0-4fe2-95fc-680546cdcbe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8946e+23, 2.1940e+17, 2.2169e+17, 2.7137e+16, 2.8946e+23, 2.1940e+17,\n",
       "         2.2169e+17, 2.7137e+16],\n",
       "        [2.1940e+17, 5.1095e+31, 5.6861e+23, 7.5390e+17, 2.1940e+17, 5.1095e+31,\n",
       "         5.6861e+23, 7.5390e+17],\n",
       "        [2.2169e+17, 5.6861e+23, 7.4910e+22, 1.4818e+20, 2.2169e+17, 5.6861e+23,\n",
       "         7.4910e+22, 1.4818e+20],\n",
       "        [2.7137e+16, 7.5390e+17, 1.4818e+20, 4.9390e+24, 2.7137e+16, 7.5390e+17,\n",
       "         1.4818e+20, 4.9390e+24],\n",
       "        [2.8946e+23, 2.1940e+17, 2.2169e+17, 2.7137e+16, 2.8946e+23, 2.1940e+17,\n",
       "         2.2169e+17, 2.7137e+16],\n",
       "        [2.1940e+17, 5.1095e+31, 5.6861e+23, 7.5390e+17, 2.1940e+17, 5.1095e+31,\n",
       "         5.6861e+23, 7.5390e+17],\n",
       "        [2.2169e+17, 5.6861e+23, 7.4910e+22, 1.4818e+20, 2.2169e+17, 5.6861e+23,\n",
       "         7.4910e+22, 1.4818e+20],\n",
       "        [2.7137e+16, 7.5390e+17, 1.4818e+20, 4.9390e+24, 2.7137e+16, 7.5390e+17,\n",
       "         1.4818e+20, 4.9390e+24]], dtype=torch.float64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = torch.exp(cov / 0.1)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52f00a83-ff94-42f7-a11f-d6a6397e7360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd5b545dc0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 413,
       "width": 408
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(sim.detach().numpy()/sim.max().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b819a3f1-4ec3-4bba-b46a-d28590015c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~torch.eye(n_samples, device='cpu').bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45fbe952-0b26-4273-b6f2-fd058491e772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [ True, False,  True,  True,  True,  True,  True,  True],\n",
       "        [ True,  True, False,  True,  True,  True,  True,  True],\n",
       "        [ True,  True,  True, False,  True,  True,  True,  True],\n",
       "        [ True,  True,  True,  True, False,  True,  True,  True],\n",
       "        [ True,  True,  True,  True,  True, False,  True,  True],\n",
       "        [ True,  True,  True,  True,  True,  True, False,  True],\n",
       "        [ True,  True,  True,  True,  True,  True,  True, False]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06a17894-7663-48e1-ad95-f908f37c38a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8946e+23, 5.1095e+31, 1.2124e+24, 4.9393e+24, 2.8946e+23, 5.1095e+31,\n",
       "        1.2124e+24, 4.9393e+24], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = sim.masked_select(mask).view(n_samples, -1).sum(dim=-1)\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02ef99a2-1571-4bb7-a543-65620f4c2952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.8946e+23, 5.1095e+31, 7.4910e+22, 4.9390e+24], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = torch.exp(torch.sum(a * a, dim=-1) / 0.1)\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b81cfa54-27f0-4e3e-b888-3b9c591f4471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7226, 0.8226, 0.1686, 0.0128, 0.4602, 0.0442, 0.8772, 0.4807, 0.0343,\n",
       "         0.1533, 0.6393, 0.0985, 0.3595, 0.0443, 0.8754, 0.3579, 0.5887, 0.1382,\n",
       "         0.4748, 0.9491],\n",
       "        [0.9278, 0.6011, 0.8120, 0.9719, 0.1510, 0.0305, 0.1976, 0.0418, 0.8332,\n",
       "         0.4206, 0.5747, 0.3835, 0.9328, 0.7024, 0.6640, 0.3037, 0.9474, 0.2863,\n",
       "         0.4265, 0.0634],\n",
       "        [0.7741, 0.5426, 0.6547, 0.2739, 0.3012, 0.0467, 0.3211, 0.2559, 0.9328,\n",
       "         0.2122, 0.2149, 0.4281, 0.7463, 0.2176, 0.5906, 0.1065, 0.5939, 0.3515,\n",
       "         0.9081, 0.4453],\n",
       "        [0.6371, 0.0719, 0.4750, 0.0307, 0.2077, 0.3757, 0.5899, 0.9960, 0.8472,\n",
       "         0.2983, 0.1232, 0.6274, 0.6610, 0.2617, 0.1144, 0.1047, 0.2864, 0.4276,\n",
       "         0.8551, 0.8848]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd3567-20bd-4955-8ab5-1d5751d0357e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1911cdec-6358-4b7d-96e9-472206acce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/app/_data/artist_data/\"\n",
    "train = pd.read_csv(os.path.join(root_dir, \"train_meta.tsv\"), sep=\"\\t\")\n",
    "test = pd.read_csv(os.path.join(root_dir, \"test_meta.tsv\"), sep=\"\\t\")\n",
    "\n",
    "fl = FeaturesLoader(\n",
    "    features_dir_path=\"/app/_data/artist_data/train_features\",\n",
    "    df=train,\n",
    "    device=\"cpu\",\n",
    "    augment=True,\n",
    "    crop_size=60,\n",
    ")\n",
    "\n",
    "tl = TrainLoader(features_loader=fl, batch_size=256, features_size=(512, 60))\n",
    "\n",
    "for i, x in enumerate(tl):\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd241353-b557-4261-8aa7-c3e5b906318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_features_size1=512,\n",
    "        output_features_size2=128,\n",
    "        embed_len=128,\n",
    "        kernel_size=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_features_size = output_features_size1 + output_features_size2\n",
    "        self.embed_len = embed_len\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            512, output_features_size1, kernel_size=kernel_size, padding=1\n",
    "        )\n",
    "        self.conv_2 = nn.Conv1d(\n",
    "            output_features_size1,\n",
    "            output_features_size1,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.mp_1 = nn.MaxPool1d(2)\n",
    "        self.conv_1_2 = nn.Conv1d(\n",
    "            60, output_features_size2, kernel_size=kernel_size, padding=1\n",
    "        )\n",
    "        self.conv_2_2 = nn.Conv1d(\n",
    "            output_features_size2,\n",
    "            output_features_size2,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.mp_1_2 = nn.MaxPool1d(2)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.output_features_size, self.output_features_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.output_features_size, self.embed_len, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.transpose(2, 1)\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.mp_1(x).mean(axis=2)\n",
    "\n",
    "        y = F.relu(self.conv_1_2(y))\n",
    "        y = F.relu(self.conv_2_2(y))\n",
    "        y = self.mp_1_2(y).mean(axis=2)\n",
    "        output = torch.cat([x, y], axis=-1)\n",
    "        output = self.linear(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "102ee34e-d7fd-478b-af8a-703d889ed6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(\n",
    "        output_features_size1=512,\n",
    "        output_features_size2=128,\n",
    "        embed_len=256,\n",
    "        kernel_size=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5718410a-f63f-46c7-9217-96168d952279",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.load('/app/_data/artist_data/models/exp1/best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3cf382b-efbd-42e8-be54-d1812b84f972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (conv_1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (mp_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv_1_2): Conv1d(60, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv_2_2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (mp_1_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=640, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=640, out_features=256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c13cd47-0377-4ffd-87eb-b7684e00d6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925f23b-cd2b-4e70-8945-6b1bfc0956c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235c4ebc-c8bf-4b4d-97b1-3e8ed202e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Loss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super(Custom_Loss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_fn = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, y_i, y_j):\n",
    "        batch_size = y_i.shape[0]\n",
    "        N = 2 * batch_size\n",
    "        z = torch.cat((y_i, y_j), dim=0)\n",
    "        sim = self.similarity_fn(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "        sim_i_j = torch.diag(sim, batch_size)\n",
    "        sim_j_i = torch.diag(sim, -batch_size)\n",
    "\n",
    "        mask = self.mask_correlated_samples(batch_size)\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N, 1)\n",
    "        negative_samples = sim[mask].reshape(N, -1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_samples.device).long()\n",
    "        labels[0] = 1\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss /= N\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def get_ranked_list(embeds, top_size=100, annoy_num_trees=128):\n",
    "    annoy_index = None\n",
    "    annoy2id = []\n",
    "    id2annoy = dict()\n",
    "    for track_id, track_embed in tqdm(embeds.items()):\n",
    "        id2annoy[track_id] = len(annoy2id)\n",
    "        annoy2id.append(track_id)\n",
    "        if annoy_index is None:\n",
    "            annoy_index = annoy.AnnoyIndex(len(track_embed), \"angular\")\n",
    "        annoy_index.add_item(id2annoy[track_id], track_embed)\n",
    "    annoy_index.build(annoy_num_trees, n_jobs=-1)\n",
    "    ranked_list = dict()\n",
    "    for track_id in tqdm(embeds.keys()):\n",
    "        candidates = annoy_index.get_nns_by_item(id2annoy[track_id], top_size + 1)[1:]\n",
    "        candidates = list(filter(lambda x: x != id2annoy[track_id], candidates))\n",
    "        ranked_list[track_id] = [annoy2id[candidate] for candidate in candidates]\n",
    "    return ranked_list\n",
    "\n",
    "\n",
    "def position_discounter(position):\n",
    "    return 1.0 / np.log2(position + 1)\n",
    "\n",
    "\n",
    "def get_ideal_dcg(relevant_items_count, top_size):\n",
    "    dcg = 0.0\n",
    "    for result_indx in range(min(top_size, relevant_items_count)):\n",
    "        position = result_indx + 1\n",
    "        dcg += position_discounter(position)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def compute_dcg(query_trackid, ranked_list, track2artist, top_size):\n",
    "    query_artistid = track2artist[query_trackid]\n",
    "    dcg = 0.0\n",
    "    for result_indx, result_trackid in enumerate(ranked_list[:top_size]):\n",
    "        assert result_trackid != query_trackid\n",
    "        position = result_indx + 1\n",
    "        discounted_position = position_discounter(position)\n",
    "        result_artistid = track2artist[result_trackid]\n",
    "        if result_artistid == query_artistid:\n",
    "            dcg += discounted_position\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def eval_submission(submission, df, top_size=100):\n",
    "    track2artist = df.set_index(\"trackid\")[\"artistid\"].to_dict()\n",
    "    artist2tracks = df.groupby(\"artistid\").agg(list)[\"trackid\"].to_dict()\n",
    "    ndcg_list = []\n",
    "    for query_trackid in tqdm(submission.keys()):\n",
    "        ranked_list = submission[query_trackid]\n",
    "        query_artistid = track2artist[query_trackid]\n",
    "        query_artist_tracks_count = len(artist2tracks[query_artistid])\n",
    "        ideal_dcg = get_ideal_dcg(query_artist_tracks_count - 1, top_size=top_size)\n",
    "        dcg = compute_dcg(query_trackid, ranked_list, track2artist, top_size=top_size)\n",
    "        try:\n",
    "            ndcg_list.append(dcg / ideal_dcg)\n",
    "        except ZeroDivisionError:\n",
    "            continue\n",
    "    return np.mean(ndcg_list)\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_features_size1=512,\n",
    "        output_features_size2=128,\n",
    "        embed_len=128,\n",
    "        kernel_size=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_features_size = output_features_size1 + output_features_size2\n",
    "        self.embed_len = embed_len\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            512, output_features_size1, kernel_size=kernel_size, padding=1\n",
    "        )\n",
    "        self.conv_2 = nn.Conv1d(\n",
    "            output_features_size1,\n",
    "            output_features_size1,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.mp_1 = nn.MaxPool1d(2)\n",
    "        self.conv_1_2 = nn.Conv1d(\n",
    "            60, output_features_size2, kernel_size=kernel_size, padding=1\n",
    "        )\n",
    "        self.conv_2_2 = nn.Conv1d(\n",
    "            output_features_size2,\n",
    "            output_features_size2,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.mp_1_2 = nn.MaxPool1d(2)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(self.output_features_size, self.output_features_size, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.output_features_size, self.embed_len, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x.transpose(2, 1)\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.mp_1(x).mean(axis=2)\n",
    "\n",
    "        y = F.relu(self.conv_1_2(y))\n",
    "        y = F.relu(self.conv_2_2(y))\n",
    "        y = self.mp_1_2(y).mean(axis=2)\n",
    "        output = torch.cat([x, y], axis=-1)\n",
    "        output = self.linear(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def inference(model, loader):\n",
    "    embeds = dict()\n",
    "    model.eval()\n",
    "    for tracks_ids, tracks_features in loader:\n",
    "        with torch.no_grad():\n",
    "            tracks_embeds = model(tracks_features)\n",
    "            for track_id, track_embed in zip(tracks_ids, tracks_embeds):\n",
    "                embeds[track_id] = track_embed.cpu().numpy()\n",
    "    return embeds\n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, min_delta=1e-6):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_metric = 0\n",
    "\n",
    "    def early_stop(self, validation_metric):\n",
    "        if validation_metric > self.min_validation_metric:\n",
    "            self.min_validation_metric = validation_metric\n",
    "            self.counter = 0\n",
    "        elif validation_metric < (self.min_validation_metric + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    val_df,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs,\n",
    "    model_path,\n",
    "    history_path,\n",
    "    top_size=100,\n",
    "    sceduler_patience=4,\n",
    "    early_stopping_patience=13,\n",
    "):\n",
    "    max_ndcg = None\n",
    "    best_epoch = 0\n",
    "    metrics = {\"train_loss\": [], \"ndcg\": []}\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.5, patience=sceduler_patience, verbose=True\n",
    "    )\n",
    "    early_stopper = EarlyStopper(patience=early_stopping_patience)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_bar = tqdm(\n",
    "            enumerate(train_loader),\n",
    "            total=train_loader._len(),\n",
    "            desc=f\"Train: epoch #{epoch + 1}/{num_epochs}\",\n",
    "        )\n",
    "        running_loss = 0\n",
    "        epoch_loss = 0\n",
    "        for n, batch in train_bar:\n",
    "            optimizer.zero_grad()\n",
    "            model.train()\n",
    "            x_i, x_j = batch[:, 0, :, :], batch[:, 1, :, :]\n",
    "            y_i = model(x_i)\n",
    "            y_j = model(x_j)\n",
    "            loss = criterion(y_i, y_j)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss = running_loss / (n + 1)\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            memory = (\n",
    "                torch.cuda.memory_reserved() / 1e9 if torch.cuda.is_available() else 0\n",
    "            )\n",
    "            train_bar.set_postfix(\n",
    "                train_loss=f\"{epoch_loss:0.4f}\",\n",
    "                lr=f\"{current_lr:0.6f}\",\n",
    "                gpu_memory=f\"{memory:0.2f} GB\",\n",
    "            )\n",
    "        metrics[\"train_loss\"].append(epoch_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"\\nValidation nDCG on epoch {}\".format(epoch + 1))\n",
    "            embeds = inference(model, val_loader)\n",
    "            ranked_list = get_ranked_list(embeds, top_size, 128)\n",
    "            val_ndcg = eval_submission(ranked_list, val_df)\n",
    "            metrics[\"ndcg\"].append(val_ndcg)\n",
    "            print(f\"ndcg = {val_ndcg} at {epoch + 1} epoch\")\n",
    "            if (max_ndcg is None) or (val_ndcg > max_ndcg):\n",
    "                print(\n",
    "                    f\"ndcg improved from {max_ndcg} to {val_ndcg}, checkpoints saved to {model_path}\"\n",
    "                )\n",
    "                max_ndcg = val_ndcg\n",
    "                best_epoch = epoch\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            scheduler.step(val_ndcg)\n",
    "            if early_stopper.early_stop(val_ndcg):\n",
    "                break\n",
    "    with open(os.path.join(history_path, \"history.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f)\n",
    "    print(f\"Best ndcg = {max_ndcg} at {best_epoch} epoch\\n\")\n",
    "\n",
    "\n",
    "def save_submission(submission, submission_path, top=100):\n",
    "    with open(submission_path, \"w\") as f:\n",
    "        for query_trackid, result in submission.items():\n",
    "            f.write(\"{}\\t{}\\n\".format(query_trackid, \" \".join(map(str, result[:top]))))\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = ArgumentParser(description=\"Simple naive baseline\")\n",
    "    parser.add_argument(\n",
    "        \"--base-dir\",\n",
    "        dest=\"base_dir\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        type=str,\n",
    "        nargs=\"?\",\n",
    "        const=\"/app/_data/artist_data/\",\n",
    "        default=\"/app/_data/artist_data/\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save-dir\", dest=\"save_dir\", action=\"store\", required=True, type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fold\",\n",
    "        dest=\"fold\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=0,\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        dest=\"batch_size\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=512,\n",
    "        type=int,\n",
    "        default=512,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n-channels\",\n",
    "        dest=\"n_channels\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=256,\n",
    "        type=int,\n",
    "        default=256,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--emb-len\",\n",
    "        dest=\"emb_len\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=128,\n",
    "        type=int,\n",
    "        default=128,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--kernel-size\",\n",
    "        dest=\"kernel_size\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=3,\n",
    "        type=int,\n",
    "        default=3,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n-epochs\",\n",
    "        dest=\"n_epochs\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=150,\n",
    "        type=int,\n",
    "        default=150,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--temperature\",\n",
    "        dest=\"temperature\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=0.01,\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--stopping-patience\",\n",
    "        dest=\"early_stopping_patience\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=13,\n",
    "        type=int,\n",
    "        default=13,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sceduler-patience\",\n",
    "        dest=\"sceduler_patience\",\n",
    "        action=\"store\",\n",
    "        required=False,\n",
    "        nargs=\"?\",\n",
    "        const=13,\n",
    "        type=int,\n",
    "        default=13,\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # python3 train_main.py --base-dir='/app/_data/artist_data/' --save-dir='ny_2_1' --fold=1 --batch-size=512 --n-channels=512 --emb-len=256 --n-epochs=150 --kernel-size=3 --temperature=0.01\n",
    "    # Seed\n",
    "    seed = 42\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    N_OUTPUT_CHANNELS = args.n_channels\n",
    "    EMBEDDING_LEN = args.emb_len\n",
    "    KERNEL_SIZE = args.kernel_size\n",
    "    NUM_EPOCHS = args.n_epochs\n",
    "    FOLD = args.fold\n",
    "    LR = 1e-4\n",
    "    TEMPERATURE = args.temperature\n",
    "    SCEDULER_PATIENCE = args.sceduler_patience\n",
    "    STOPPING_PATIENCE = args.early_stopping_patience\n",
    "\n",
    "    TRAINSET_PATH = os.path.join(args.base_dir, \"train_features\")\n",
    "    TESTSET_PATH = os.path.join(args.base_dir, \"test_features\")\n",
    "    TRAINSET_META_PATH = os.path.join(args.base_dir, \"train_meta.tsv\")\n",
    "    TESTSET_META_PATH = os.path.join(args.base_dir, \"test_meta.tsv\")\n",
    "    BASE_DIR = os.path.join(args.base_dir, \"models\", args.save_dir)\n",
    "    SUBMISSION_PATH = os.path.join(BASE_DIR, \"submission.txt\")\n",
    "    CHECKPOINT_PATH = os.path.join(BASE_DIR, \"best.pt\")\n",
    "\n",
    "    if not os.path.exists(BASE_DIR):\n",
    "        os.makedirs(BASE_DIR)\n",
    "\n",
    "    with open(\n",
    "        os.path.join(args.base_dir, \"models\", args.save_dir, \"args.json\"), \"w\"\n",
    "    ) as f:\n",
    "        json.dump(args.__dict__, f)\n",
    "\n",
    "    model = BaseModel(N_CHANNELS, 128, EMBEDDING_LEN).to(DEVICE)\n",
    "\n",
    "    train = pd.read_csv(TRAINSET_META_PATH, sep=\"\\t\")\n",
    "    test_df = pd.read_csv(TESTSET_META_PATH, sep=\"\\t\")\n",
    "    train_df, val_df = train_val_split(train, fold=FOLD, seed=seed)\n",
    "\n",
    "    print(\"Loaded data\")\n",
    "    print(\"Train set size: {}\".format(len(train_df)))\n",
    "    print(\"Validation set size: {}\".format(len(val_df)))\n",
    "    print(\"Test set size: {}\".format(len(test_df)))\n",
    "    print()\n",
    "    print(\"Train\")\n",
    "    train(\n",
    "        model=model,\n",
    "        train_loader=TrainLoader(\n",
    "            FeaturesLoader(\n",
    "                features_dir_path=TRAINSET_PATH,\n",
    "                meta_info=train_meta_info,\n",
    "                device=DEVICE,\n",
    "                augment=True,\n",
    "            ),\n",
    "            batch_size=BATCH_SIZE,\n",
    "        ),\n",
    "        val_loader=TestLoader(\n",
    "            FeaturesLoader(\n",
    "                features_dir_path=TRAINSET_PATH,\n",
    "                meta_info=validation_meta_info,\n",
    "                device=DEVICE,\n",
    "                augment=False,\n",
    "            ),\n",
    "            batch_size=BATCH_SIZE,\n",
    "        ),\n",
    "        valset_meta=validation_meta_info,\n",
    "        optimizer=torch.optim.Adam(sim_clr.parameters(), lr=LR),\n",
    "        criterion=Custom_Loss(temperature=TEMPERATURE),\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        model_path=CHECKPOINT_PATH,\n",
    "        history_path=BASE_DIR,\n",
    "        sceduler_patience=SCEDULER_PATIENCE,\n",
    "        early_stopping_patience=args.early_stopping_patience,\n",
    "    )\n",
    "\n",
    "    print(\"\\nSubmission\")\n",
    "    test_loader = TestLoader(\n",
    "        FeaturesLoader(\n",
    "            features_dir_path=TESTSET_PATH,\n",
    "            meta_info=test_meta_info,\n",
    "            device=DEVICE,\n",
    "            augment=False,\n",
    "        ),\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH)).to(DEVICE)\n",
    "    model.eval()\n",
    "    embeds = inference(model, test_loader)\n",
    "    submission = get_ranked_list(embeds=embeds, top_size=500, annoy_num_trees=1024)\n",
    "    save_submission(submission, os.path.join(BASE_DIR, \"submission_100.txt\"), top=100)\n",
    "    save_submission(submission, os.path.join(BASE_DIR, \"submission_500.txt\"), top=500)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
